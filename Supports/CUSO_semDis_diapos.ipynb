{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43815e08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction à la sémantique distributionnelle\n",
    "## École doctorale CUSO\n",
    "\n",
    "<span style=\"color:tan\"> Marine Wauquier</span>\n",
    "\n",
    "<span style=\"color:tan\">*MCF Sorbonne Nouvelle - Lattice (CNRS/SN/ENS)*</span>\n",
    "\n",
    "<!--\n",
    "<img style=\"float: left;\" src=\"media/logo_sn.png\" width=\"100\">\n",
    "<img style=\"float: left;\" src=\"media/logo_lattice.jpg\" width=\"100\">\n",
    "<img style=\"float: left;\" src=\"media/logo_ens_psl.png\" width=\"100\">\n",
    "<img style=\"float: left;\" src=\"media/logo_cnrs.jpg\" width=\"50\">\n",
    "!-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a3e9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Sur le plan théorique,\n",
    "\n",
    "* L'objectif de cette présentation est de se familiariser avec la sémantique distributionnelle\n",
    "    * comprendre l'utilité et l'origine de cette approche\n",
    "    * découvrir dans les grandes lignes son fonctionnement\n",
    "    * s'initier à l'utilisation des outils de sémantique distributionnelle\n",
    "    * réfléchir à l'intégration de la sémantique distributionnelle dans sa propre pratique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540b2e8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## Introduction\n",
    " \n",
    " Sur le plan pratique,\n",
    " \n",
    " * Cette présentation se veut interactive, mêlant théorie et pratique\n",
    " * Nous manipulerons pour cela un notebook Google Colab'\n",
    "     * Le notebook est accessible en ligne [en cliquant ici](https://colab.research.google.com/drive/11SbV3IZ-9shfg95mHP9Oa19M3jboFI6L?usp=sharing)\n",
    "     * Le Drive contenant les différentes ressources est accessible en ligne en [en cliquant ici](https://drive.google.com/drive/folders/1ljH-oIh8d156x8Ketoa7mrR9_Qsn1l7O?usp=sharing)\n",
    "     * Les diapositives interactives sont aussi accessibles en ligne sur une [page dédiée de mon GitHub](https://github.com/mwauquier/CUSO_dsm_tutorial).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ffd6c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pourquoi la sémantique distributionnelle ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306c1eda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> ### Expérience de pensée\n",
    ">\n",
    "> Je viens de descendre du train à la gare de Neuchâtel, et je veux manger japonais. Je sors donc mon smartphone de ma poche, et je tape \"resto maki\" dans mon moteau de recherche.\n",
    "> \n",
    "> Quel(s) résultats le moteur de recherche est-il le plus susceptible de me retourner ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9be6c4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. **Restaurant japonais -** Tous les restaurants à proximité, avec une large sélection de sushi et yakitori...\n",
    "2. **Maki Namekawa -** Formée entre le Japon et l'Europe, Maki Namekawa est une pianiste ambitieuse...\n",
    "3. **\"*Pas de sushis!*\" -** Dégustez à volonté des spécialités thaï, chinoises et japonaises...\n",
    "4. **Presto - Making-of -** Sortie incontournable de l'été, le film aux 5 Oscars se dévoile au travers d'un making-of..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b51f90",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Recherche documentaire comme clé\n",
    "\n",
    "La recherche documentaire est à l'origine des implémentations modernes des systèmes d'embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fa9192",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Quels documents (*i.e.* ensemble de mots) sont le plus similaires à une requête donnée (*i.e.* ensemble de mots) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ee4791",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deuxième expérience de pensée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee86615",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Imaginons que vous assistez à un workshop CUSO, et que Lucie Barque vous présente une étude sur la régularité des patrons de polysémie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfddfe48",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* L'autrice veut mettre à l'épreuve l'hypothèse que les sens d'un patron métonymique (ANIMAL $>$ sont plus *proches* que les sens d'un patron métaphorique (ANIMAL $>$ PERSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e1698",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Comment mesurer et comparer cette *proximité sémantique* ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddefa98f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Troisième expérience de pensée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8147574",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Imaginons que vous assistez à un workshop CUSO, et qu'Alain Kamper vous présente l'outil d'exploration du Corpus français Leipzig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09550d2f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    " <img src=\"media/leipzig_distrib.png\"  alt=\"Leipzig Corpus\" width=700/>\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6235b19",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Comment interpréter ou construire la section \"Mots avec un contexte similaire\" ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fca8270",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quésaco ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaed87d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    " <img src=\"media/schtroumpf.png\"  alt=\"schtroumpf\" width=700/>\n",
    "<center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03310edd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Dans cet extrait, quels sont la catégorie grammaticale et le sens des deux occurrences de *schtroumpfs* ?\n",
    "    * Qu'est-ce qui vous permet de dire ça ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b1299b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hypothèse distributionnelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afcaedc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> You shall know a word by the company it keeps *(Firth 1957:11)*\n",
    "\n",
    "* Le sens d'un mot est accessible par le biais de ses contextes (Harris 1954, Firth 1957)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62627a0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> the amount of meaning difference correspond[s] roughly to the amount of difference in their environments *(Harris 1954:157)*\n",
    "\n",
    "* Des mots similaires tendent à apparaître dans des contextes similaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83910b35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* La proximité distributionnelle est vue comme un indice de proximité sémantique des mots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe6d9a4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* L'hypothèse n'avait pas à l'origine une portée sémantique\n",
    "    * Bloomfield a développé l'approche distributionnelle pour discrétiser *a priori* les catégories grammaticales dans des langues inconnues sur la base de schémas de cooccurrences (*i.e.* de comportements syntaxiques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40297ce8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## De l'hypothèse aux modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db0f2a6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Comment est calculée cette proximité distributionnelle ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04226d88",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* L'approche distributionnelle propose de représenter le 'sens' des mots sous la forme de **vecteurs** (ou ***embeddings***)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8087fb86",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Pour rappel, un vecteur est :\n",
    "    * un objet mathématique qui possède une longueur, une direction, et un sens\n",
    "    * représenté par une flèche reliant un point de départ et un point d'arrivée (indiqués par des coordonnées)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16569c9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    " <img src=\"media/arrow.png\"  alt=\"vector\" width=500/>\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53d64ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    " <img src=\"media/arrow_bg.png\"  alt=\"vector space\" width=500/>\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2147a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Qu'est-ce qu'un vecteur distributionnel (*word embedding*) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d03b5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* C'est la synthèse des environnements linguistiques (ou comportements syntagmatiques) d'une unité lexicale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ccebf2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Son calcul repose (globalement) sur la quantification des cooccurrences de l'unité"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c2439",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Soit le corpus fictif\n",
    "\n",
    "> The committee asknowledged the **researcher**'s thoughtful work...\n",
    "> \n",
    "> In her new album, the **singer** offers yet another beautiful song...\n",
    ">\n",
    "> In the very next scene, the main character is introduced to Myung-sook, a beautiful **dancer** who fall in love of him...\n",
    ">\n",
    "> ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e475637f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* On peut extraire de ce corpus la matrice de cooccurrences suivante :\n",
    "\n",
    "\n",
    "|   | *beautiful*  | *thoughtful*  |\n",
    "|---|---|---|\n",
    "| ***researcher***  | 2  | 10  |\n",
    "| ***singer***  | 10  | 1  |\n",
    "| ***dancer***  | 7  | 3  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1be7001",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Si l'on considère que les deux contextes *beautiful* et *thoughtful* constituent l'abscisse et l'ordonnée respectivement, cette matrice nous indique que le vecteur de *researcher* a pour coordonnées (2,10), le vecteur de *singer* les coordonnées (10,1) et le vecteur de *dancer* (7,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c03d066",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* On peut représenter donc la matrice de la façon suivante\n",
    "\n",
    "<center>\n",
    " <img src=\"media/fig1_bg.png\"  alt=\"DSM with researcher, singer and dancer\" width=500/>\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c737fcb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Cette figure est la représentation d'un espace vectoriel à deux dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407bf31a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Espace vectoriel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f741ec5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Chaque mot du corpus fait l'objet d'un vecteur (ou word embedding)\n",
    "    * Nous reviendrons sur la notion de \"mot\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec29ca2d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Chaque contexte constitue une dimension du vecteur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b16bb1b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* L'ensemble des vecteurs forme un espace vectoriel (appelé DSM pour *Distributional Semantics Model*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab8da52",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Noter que l'espace vectoriel est intrinsèquement dépendant du corpus\n",
    "    * Le vecteur du mot *chien* n'est pas absolu, et varie en fonction du corpus que l'on considère"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb031001",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Score de proximité distributionnelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc5d295",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Il existe deux façons de calculer la proximité distributionnelle\n",
    "    * **Distance euclidienne**, *i.e.* la distance entre les deux vecteurs\n",
    "    * **Distance cosinus**, *i.e.* l'angle entre les deux vecteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca134e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    " <img src=\"media/fig1_bg.png\"  alt=\"DSM with researcher, singer and dancer\" width=500/>\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60812e61",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Score de proximité distributionnelle\n",
    "\n",
    "* La distance euclidienne est plus sensible à la longueur des vecteurs que la distance cosinus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8385bfed",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Le score le plus souvent utilisé est celui calculé à partir de la distance cosinus \n",
    "    * Pertinence de la méthode semble varier en fonction des tâches et des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187f211",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Dans les deux cas, on obtient un score qui va de 0 (proximité nulle) à 1 (proximité maximale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40940957",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calcul du score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d92db39",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Le score de proximité pour les vecteurs *v* et *w* se calcule selon la formule :\n",
    "\n",
    "$$ prox(v,w) = \\frac{v\\cdot w}{|v||w|} = \\frac{\\sum_{i=1}^n v_i w_i}{\\sqrt{\\sum_{i=1}^n v^2_i}\\sqrt{\\sum_{i=1}^n w^2_i}}$$\n",
    "\n",
    "où $v\\cdot w$ est le produit scalaire des vecteurs v et w, et $|v||w|$ le produit de leur norme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461bc8c3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Le produit scalaire est obtenu en faisant la somme de la multiplication des dimensions 2 à 2\n",
    "* La norme est obtenue en faisant la racine carrée de la somme des dimensions au carré"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fdfc59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calcul du score\n",
    "\n",
    "Si l'on reprend notre matrice de cooccurrences\n",
    "\n",
    "|   | *beautiful*  | *thoughtful*  |\n",
    "|---|---|---|\n",
    "| ***researcher***  | 2  | 10  |\n",
    "| ***singer***  | 10  | 1  |\n",
    "| ***dancer***  | 7  | 3  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525da510",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Le score de proximité pour les vecteurs de *researcher* et *singer* est donc :\n",
    "\n",
    "$$ prox(researcher,singer) = \\frac{(2*10) + (10*1)}{\\sqrt{2^2 + 10^2}*\\sqrt{10^2 + 1^2}} = 0.293$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad83891",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application\n",
    "\n",
    "* Calculons maintenant le score de proximité pour *researcher* et *dancer*, et pour *singer* et *dancer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b495cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ prox(researcher,dancer) = \\frac{(2*7) + (10*3)}{\\sqrt{2^2 + 10^2}*\\sqrt{7^2 + 3^2}} = 0.567$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727d353",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ prox(singer,dancer) = \\frac{(10*7) + (1*3)}{\\sqrt{10^2 + 1^2}*\\sqrt{7^2 + 3^2}} = 0.954$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90207b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interprétation du score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb5ef5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Pour rappel, le score peut aller de 0 à 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d688ee1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Mais qu'est-ce que cela veut dire d'avoir un score de 0.567 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f49598b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Le score de proximité ne s'utilise pas tant dans l'absolu mais plutôt par comparaison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37164d5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "On peut ainsi dire que les vecteurs de *singer* et *dancer* sont plus proches (0.954) que ceux de *dancer* et *researcher* (0.567) et de *singer* et *researcher* (0.293)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423b1827",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intérêt d'un score de proximité"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97f1ea6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Ce score permet de quantifier sur une échelle de 0 à 1 la similarité sémantique de deux mots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4317e7d6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Dans notre corpus, **dancer** est sémantiquement plus proche de **singer** que de **researcher**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04797c28",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Cela permet aussi par extension de dresser la liste des *n* mots les plus sémantiquement proches\n",
    "    * Ces mots sont appelés les voisins distributionnels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddb666",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Les 5 mots les plus proches sémantiquement de *dancer* sont :\n",
    "> * ballerina 0.952\n",
    "> * choregrapher 0.923\n",
    "> * performer 0.911\n",
    "> * artist 0.894\n",
    "> * showman 0.887"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f624f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# À VOS CLAVIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aaa3b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pratiquons un peu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021dd27d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Nous utiliserons ici Word2Vec, qui est un système diffusé depuis 2013 et qui a été rapidement démocratisé\n",
    "    * Facile d'usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafecf3b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* C'était initialement du [code C+](https://code.google.com/archive/p/word2vec/) qu'il fallait télécharger et compiler sur sa machine, mais a depuis été implémenté de façon que peut directement être mobilisé dans du code sans manipulations préalables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4ef4e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Nous allons pour cela jouer avec le [Notebook Google Colaboration](https://colab.research.google.com/drive/11SbV3IZ-9shfg95mHP9Oa19M3jboFI6L?usp=sharing), que nous allons décrire pas à pas\n",
    "    * Nous allons pour le moment parcourir la **Section 1 - Manipulation d'espaces vectoriels**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccd76f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word2Vec et gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb0229",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Nous allons utiliser une implémentation Python de Word2Vec, accessible via une librairie (i.e. un ensemble de ressources et fonctions) appelée `gensim`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b4cc0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pour ce faire, on va importer, c'est-à-dire charger, la librairie en question dans notre code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0f1277",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.models.keyedvectors as word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b6a63",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Notez que si vous travaillez localement (et non en ligne sur Google Colab'), il vous faudra au préalable installer la librairie (c'est-à-dire la télécharger sur votre ordinateur)\n",
    "    * La méthode d'installation dépendra alors des caractéristiques de votre système\n",
    "    * Vous trouverez en ligne des tutoriels pour vous aider (sur [Linux en Python](https://pypi.org/project/gensim/), ou via [Anaconda](https://anaconda.org/anaconda/gensim) par exemple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486d5e34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Chercher la proximité distributionnelle de deux mots revient à comparer leurs vecteurs respectifs dans un espace vectoriel donné"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76a24ce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* On doit donc charger l'espace vectoriel qui nous intéresse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd75733",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Trois solutions s'offrent à nous :\n",
    "    * charger un modèle qui se trouve en ligne\n",
    "    * charger un modèle qui se trouve localement sur votre machine\n",
    "    * utiliser un modèle que l'on vient d'entraîner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c36242f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Charger un modèle en ligne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d3d59",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Le module `gensim.downloader` nous permet d'accéder aux modèles en ligne\n",
    "    * Comme précédemment, pour l'utiliser, nous devons le charger, c'est-à-dire donner accès à son contenu à notre code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b4c778",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7944c2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* On peut interroger le module pour voir les modèles accessibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b5b9c6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcab0be6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Il nous suffit alors d'importer le modèle de notre choix en indiquant le nom du modèle dans la commande `gensim.downloader.load()`\n",
    "    * Attention, ça peut être long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e1069c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a0931d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "https://github.com/Ismailhachimi/French-Word-Embeddings\n",
    "\n",
    "https://textminingonline.com/tag/french-word2vec pour interroger une base de donnée en ligne https://wordsimilarity.com/fr/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687cbca1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Charger un modèle local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239b5386",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Il se peut que vous ayez déjà entraîné un modèle, ou que l'on vous ait transmis un modèle entraîné\n",
    "    * La méthode de chargement dépendra de la méthode de création du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d0acca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Si le modèle a été entraîné à partir de l'outil C+ originel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d4165",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('/path/to/my/vectors.txt', binary=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547c41e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Vous pouvez aussi directement intégrer une archive du modèle (si ce dernier est trop lourd pour être décompressé), et utiliser un fichier au format binaire et non textuel (binary = True ou False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47399fb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('/path/to/my/vectors.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3792188e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Si le modèle a été entrainé avec `gensim` puis sauvegardé localement sur votre machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbd7905",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('/path/to/my/vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1867a835",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Utiliser un modèle tout frais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fc8d26",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Nous verrons plus tard comment entraîner un DSM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56815443",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Sachez néanmoins que vous pourrez l'utiliser au même titre que n'importe quel autre modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5eda8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploitation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaa089d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Maintenant que le modèle est chargé, vous pouvez l'utiliser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc0e1c0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* L'opération la plus simple consiste à chercher le score de proximité entre deux mots, à l'aide de la fonction `similarity()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f2ad2f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.similarity('cat', 'car')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b65d8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "où MODEL correspond au nom de votre modèle (`model` dans notre cas), et WORD1 et WORD2 aux mots que vous souhaitez évaluer\n",
    "* N'oubliez pas les guillemets autour des mots que vous cherchez !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb045cf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Vous pouvez aussi récupérer les voisins distributionnels d'un mot cible à l'aide de la fonction `most_similar()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82133bfb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.most_similar('cat', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8de7b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "où MODEL correspond au nom de votre modèle (`model` dans notre cas), et WORD au mot cible, et topn le nombre de voisins que vous souhaitez afficher. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db08a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Vous pouvez évidemment si vous le voulez récupérer le vecteur d'un mot (pour le réinjecter dans une autre chaîne de traitement par exemple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0639548",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vec = model['cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50e603",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Le vecteur en lui-même ne vous dira cependant pas grand chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa10e437",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ce67b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* En complément de ces fonctions de base, on trouve quelques fonctionnalités rigolotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c354f7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* On a notamment la possibilité de jouer à trouver l'intrus (sur la base des propriétés distributionnelles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea5e8a1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(model.doesnt_match(['man', 'woman', 'child', 'baby', 'car']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2557ba",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "print(model.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e680c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* On peut aussi chercher l'élément manquant d'une analogie, c'est-à-dire l'élément qui ne se conforme pas à la relation de référence\n",
    "    * L'exemple canonique étant d'identifier que *queen* est à *woman* ce que *king* est à *man*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b18428",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* L'idée est ici de transformer l'analogie en une opération arithmétique\n",
    "\n",
    "$$ king - man + woman = queen $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee15b30",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.most_similar(negative=['man'],positive=['king', 'woman'],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad37cc4c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def pp(obj):\n",
    "    print(pd.DataFrame(obj))\n",
    "    \n",
    "def analogy(worda, wordb, wordc):\n",
    "    result = model.most_similar(negative=[worda], \n",
    "                                positive=[wordb, wordc])\n",
    "    return result[0][0]\n",
    "\n",
    "countries = ['australia', 'canada', 'germany', 'ireland', 'italy']\n",
    "foods = [analogy('us', 'hamburger', country) for country in countries]\n",
    "pp(zip(countries,foods))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec0420b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Crash test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1ea670",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Que se passe-t-il si l'on cherche les voisins distributionnels de 'stock' ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34310d78",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.most_similar('stock', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d377b68",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Le modèle distributionnel dépend du corpus sur lequel il a été entraîné"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab13deb3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Les relations sémantiques en jeu sont donc spécifiques au corpus \n",
    "    * Les voisins les plus proches varieront d'un modèle à l'autre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca4042",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Et quel est le voisinage distributionnel de 'schtroumpf' ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad8e5eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.most_similar('schtroumpf', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c46b7d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* De fait, il ne peut donner d'informations que pour les mots contenus dans le corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d042e46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Crash test 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b405542",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Quelle est la similarité de *good* et *bad*, *dog* et *beagle*, et *game* et *play* ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0715a0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(model.similarity('good', 'bad'))\n",
    "print(model.similarity('dog', 'beagle'))\n",
    "print(model.similarity('game', 'play'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393d5a6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Quels sont les voisins de *mouse* ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc63ebc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.most_similar('mouse', topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c67858",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* La proximité sémantique distributionnelle recouvre en réalité plein de phénomènes sémantiques\n",
    "    * Synonymie, antonymie, hypéronymie, champs lexicaux..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5591af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * Le score distributionnel couvre en réalité la notion de *relatedness* plutôt que de similarité"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba95f1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Ce mode de représentation est par ailleurs limité par des phénomènes linguistiques fréquents tels que la polysémie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810255f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# One hypothesis to distribute them all?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1631c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Un espace (in)fini ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0629f782",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Pour rappel, notre exemple construit comportait 3 cibles (*researcher*, *singer* et *dancer*) et se basait sur deux contextes/dimensions\n",
    "\n",
    "|   | *beautiful*  | *thoughtful*  |\n",
    "|---|---|---|\n",
    "| ***researcher***  | 2  | 10  |\n",
    "| ***singer***  | 10  | 1  |\n",
    "| ***dancer***  | 7  | 3  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f5b8a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Il était une illustration (en taille réduite) des modèles dits *count-based*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c72c47",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Dans cette approche *count-based*, un espace vectoriel comporte virtuellement autant de dimensions que de mots contenus dans le corpus à partir duquel il est construit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7382a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Un espace (in)fini ?\n",
    "\n",
    "* Cela a donc deux conséquences (potentiellement) problématiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a37756",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|   | *beautiful*  | *thoughtful*  | *cat* | *dress* | *subliminal* | *hamburger* | *the*| ...|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "| ***researcher***  | 2  | 10  |3|1|0|0|10|...|\n",
    "| ***singer***  | 10  | 1  |0|5|0|1|11|...|\n",
    "| ***dancer***  | 7  | 3  |0|4|1|1|9|...|\n",
    "| ***adenovirus***  | 0  | 0  |0|0|0|0|12|...|\n",
    "| ***obambulate***  | 0  | 0  |0|0|0|0|0|...|\n",
    "| ...  | ...  | ...  |...|...|...|...|...|...|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e65f10",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* On a une matrice de très grande dimension (donc lourde)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b370cf5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Certains vecteurs sont 'creux' (*sparse*)\n",
    "    * Leurs coordonnées correspondent très largement à des 0\n",
    "    * Ils ne comportent que peu d'informations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2b248d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vers un espace fini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c33495",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Pour limiter ces problèmes, diverses méthodes ont été développées pour réduire la taille et optimiser les matrices\n",
    "    * Elles visent à compacter, condenser l'information distributionnelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d44d1b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Certaines interviennent *a posteriori* sur les modèles *count-based*\n",
    "    * On parle d'opérations de réduction de dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c17100b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Certaines permettent aussi de créer en amont des matrices plus condensées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6656b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* C'est notamment le cas des modèles dits prédictifs\n",
    "    * Word2Vec en est un des représentant les plus connus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed5875",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modèles prédictifs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1428a0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Ces modèles sont généralement construit à partir d'un réseau de neurones\n",
    "    * À l'image de Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b5a9c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Les coordonnées des vecteurs ne sont pas directement calculées à partir d'un décompte des occurrences, mais apprises par apprentissage automatique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41328db9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Le système initialise au départ la matrice avec des valeurs aléatoires\n",
    "    * Pour un mot donné, les valeurs sont actualisées au regard de son contexte\n",
    "    * Elles sont actualitées de sorte que deux vecteurs apparaissant dans des contextes similaires aient des coordonnées similaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bf63e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* De fait, on peut créer une matrice avec un nombre moindre de dimensions, et donc plus légère"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe200d9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Les dimensions ne correspondent cependant plus à des contextes linguistiques identifiés\n",
    "    * Des travaux se penchent sur l'interprétation des dimensions, mais ce n'est pas encore convaincant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcd082e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Il y a donc un certain effet \"boîte noire\" en cela que l'on ne sait pas précisément les généralisations faites par le système"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dd9ceb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Ces modèles et leurs représentations sont particulièrement dépendants d'hyper-paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3539b4f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyper-paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e8de6a",
   "metadata": {},
   "source": [
    "* Les hyper-paramètres sont une façon de moduler la façon dont les coordonnées des vecteurs sont apprises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09604ae",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Parmi les différents hyper-paramètres, on retrouve :\n",
    "    * L'architecture\n",
    "    * Le nombre de dimensions\n",
    "    * Le seuil de fréquence\n",
    "    * La taille de la fenêtre\n",
    "    * Le sub sampling\n",
    "    * Le negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15002799",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be2b2b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* L'architecture correspond (grosso modo) à la structure du système, à la façon dont il va apprendre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3306660",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Word2Vec compte deux architectures :\n",
    "    * CBOW (Continuous Bag of Word)\n",
    "    * SkipGram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1486f46",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Dans les deux cas, l'objectif est de prédire quelque chose sur la base d'autre chose\n",
    "    * Un mot à partir d'un contexte (CBOW)\n",
    "    * Un contexte à partir d'un mot (Skipgram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47941c65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Le nombre de dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2dc3b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Le nombre de dimensions correspond au nombre de colonnes dans la matrice, au nombre de coordonnées des vecteurs\n",
    "    * Un nombre de dimensions réduit se traduit par une information condensée\n",
    "    * Le modèle est de fait plus léger, mais l'information est donc (parfois trop) écrasée\n",
    "    * Mais mettre un (trop) grand nombre de dimensions revient à diluer l'information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7755717e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Par exemple, vaut-il mieux classer toutes les couleurs du monde en 3 ou 20 catégories ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32937db5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Le choix dépend en réalité de la tâche, de la taille du corpus, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea59652",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Taille de la fenêtre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0049e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* On se base sur le contexte pour mettre à jour les valeurs des vecteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aed359",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Mais qu'est-ce qu'un contexte ?\n",
    "    * La phrase entière ?\n",
    "    * Tout ce qui suit le mot cible ?\n",
    "    * Ce qui précède immédiatement le mot cible ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d0e9c0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* On doit donc dire au système combien de mots autour de notre mot cible sont à prendre en compte\n",
    "    * Plus la fenêtre est grande, plus on prend en compte d'éléments dans la distribution\n",
    "    * Mais plus la fenêtre est grande, plus on bruite la distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024feb49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Seuil de fréquence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450ef15f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* L'apprentissage machine requiert de grandes quantités de données\n",
    "    * On parle de corpus de milliards de mots\n",
    "    * Même en réduisant le nombre de dimensions, on se retrouve avec des espaces vectoriels très larges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09215a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Par ailleurs, la loi de Zipf implique qu'un grand nombre de mots ont une fréquence très faible\n",
    "    * Ils apparaissent dans un nombre très faible de contextes\n",
    "    * Leur distribution est donc peu informative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b816d3e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*  On fixe donc un seuil de fréquence minimum pour éviter d'avoir trop de vecteurs qui soient peu pertinents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04e8e80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976fced7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* À l'inverse, certains mots sont excessivement fréquents\n",
    "    * *le, la, des, à*..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a45cda",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Leur présence dans le contexte d'un mot n'est que peu informative, puisqu'ils se trouvent par défaut dans le contexte d'un très grand nombre de mots\n",
    "    * Ils ne sont pas discriminants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be0b0ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* On neutralise donc ses mots les plus fréquents en supprimant aléatoirement certains de ces mots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f1870a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cc2917",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Le système apprend à partir des exemples qu'il voit en corpus\n",
    "    * Il ne voit donc que des cas \"vrais\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e8d476",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Mais on apprend aussi (mieux) de nos erreurs\n",
    "    * On le force donc aussi à apprendre à partir d'exemples qu'il ne voit pas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18714ec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Pour cela, on crée des contextes erronés (en lui indiquant qu'ils sont erronnés)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f34341",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Et tout le reste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e91673a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* En plus de ces paramètres techniques, l'apprentissage de nos vecteurs peut être influencé par d'autres facteurs\n",
    "    * Ces facteurs se réalisent notamment à l'échelle du corpus et de son pré-traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d871bcdf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* La lemmatisation\n",
    "    * Veut-on que *mangent* et *manger* soient deux vecteurs distincts ou pas ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c701c4e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* POS-tagging\n",
    "    * Veut-on que *petit* dans *il caresse le petit chien* et dans *la mère guide son petit jusqu'au terrier* soit deux vecteurs distincts ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b314298",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Désambiguisation\n",
    "    * Veut-on que *vol* dans *le vol du joyau royal a choqué le grand public* et dans *le vol de la galinette cendrée est rarement observable* soient deux vecteurs distincts ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74861e62",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# À VOS CLAVIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815e2aaf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entraînons-nous un peu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f54d0e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "http://w3.erss.univ-tlse2.fr/UETAL/2018-2019/emb/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aab79d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Entraîner un modèle vectoriel est relativement simple (avec `gensim`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813a7987",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Le système prend en entrée des phrases (en réalité, des listes de mots), et retourne une série de vecteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc90c05",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Dans les faits, nous lui fournissons fichier texte (.txt), où chaque ligne correspond à une phrase et où chaque mot est tokenisé (séparé par des espaces blancs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09c3cf7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Pour commencer, nous utiliserons des corpus disponibles en ligne dans le Project Gutenberg (SECTION 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aa99fc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Pour cela, nous importons et chargeons les librairies nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9272d8e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238a3001",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* De nombreux textes y sont disponibles au format texte.\n",
    "    * Choisissez celui sur lequel vous voulez travaillez parmi la liste exhaustive présentée dans le notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cacbbd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Le système ne prend cependant pas un texte brut (non traité), mais des listes de mots\n",
    "    * Nous devons donc appliquer un minimum de traitement à notre corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee85260",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = gutenberg.raw('austen-emma.txt')\n",
    "print(text[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657c18e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* La librairie `nltk` propose directement une fonction permettant de traiter ses corpus, `gutenberg.sents()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60329684",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "corpus = gutenberg.sents('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc3e86",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "où XX est le nom du corpus que vous avez choisi.\n",
    "* En l'occurrence, nous ferons ici le choix de travailler sur le corpus 'austen-emma.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dee65e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Il ne reste plus qu'à entraîner le modèle à l'aide de la fonction `gensim.models.Word2Vec()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967fe2c2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_default = gensim.models.Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc14c4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Votre modèle est ici enregistré dans la variable *model_default*\n",
    "    * Vous pouvez ainsi directement la mobiliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacc0a02",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_default.wv.similarity(\"Emma\",\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62dd95d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Notez ici l'utilisation du préfixe *wv* pour appeler la fonction\n",
    "    * Cela est notamment dû aux différentes fonctions d'entraînement de Word2Vec avec `gensim`\n",
    "    * Notez que cela ne s'utilise que si vous travaillez sur un modèle que vous venez tout juste d'entraîner dans votre instance, pas avec un modèle que vous importez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68449ef5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Notez par ailleurs, comme son nom l'indique, que vous avez ici entraîné un modèle par défaut\n",
    "    * Nous n'avons renseigné aucun paramètre autre que le corpus\n",
    "    * Les hyper-paramètres ont donc des valeurs par défaut\n",
    "    * Dimensions des vecteurs = 100, fenêtre de 5, seuil de fréquence minimale de 5, CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa30ba6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Vous pouvez modifier ces valeurs en ajoutant des arguments dans la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88527edf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_advanced = gensim.models.Word2Vec(corpus, min_count=50, vector_size=200, window=3, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7045159b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Cette commande indique que l'on veut entraîner un modèle sur notre corpus avec un seuil de fréquence de 50, des vecteurs à 200 dimensions, une fenêtre de 3, et l'utilisation de l'architecture SkipGram\n",
    "    * Mais vous pouvez évidemment faire varier les valeurs !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f3e582",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Vous pouvez aussi, si vous le souhaitez, entraîner un modèle à partir du petit corpus présent dans le Drive associé à ce notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9cbb8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Cela nécessite cependant au préalable d'accorder au notebook l'autorisation d'accéder au dossier Drive\n",
    "    * Notez que le code en question de modifiera rien sur votre Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33970ff7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114c87d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Il suffit alors d'utiliser la fonction `LineSentence()` de `gensim` (similaire à `guntenberg.sents()`) sur le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa08f9a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import LineSentence\n",
    "from itertools import islice\n",
    "\n",
    "corpus_bnc = LineSentence('/content/drive/MyDrive/CUSO_2022_semantique_distributionnelle/tiny_sample_bnc.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d4f76e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e699c4f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Vous pouvez alors entraîner le modèle, comme précédemment, avec ou sans paramètres spécifiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd51fe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_default_bnc = gensim.models.Word2Vec(corpus_bnc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea87e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Vous pouvez dès lors comparer les représentations construites sur les différents corpus et/ou avec les différents paramètres\n",
    "    * Quels sont dans ces modèles les voisins de *man* ? Et de *stock* ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd938c2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Pour le moment, le modèle n'est enregistré que dans votre variable\n",
    "    * Si vous fermez le programme, votre modèle disparaît\n",
    "    * Vous n'aurez jamais strictement le même modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ace08f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Vous pouvez sauvegarder et exporter votre modèle avec la fonction `save()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c7d9ae",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "word_vectors = model_default.wv\n",
    "word_vectors.save(\"word2vec_default.wordvectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd38d58a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Le modèle peut alors être rechargé dans un programme avec la fonction `load()` vue précédemment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af655d7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('word2vec_default.wordvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d69acdf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vers l'espace vectoriel et l'au-delà"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5848d5d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Au delà de l'embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd2c8c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Les *word embeddings* sont avant tout une représentation mathématique des propriétés distributionnelles des mots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d766dec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* À ce titre, ils peuvent être manipulés bien au-delà du simple calcul de similarité"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88224ea8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compositionnalité"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c81e151",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* On peut représenter le vecteur d'un élément complexe comme la somme des vecteurs des éléments qui le composent\n",
    "    * $\\overrightarrow{voiture} + \\overrightarrow{voiture} = \\overrightarrow{voiture~rouge} $\n",
    "    * $\\overrightarrow{artisan} + \\overrightarrow{boucher} = \\overrightarrow{artisan-boucher} $\n",
    "    * $\\overrightarrow{chanter} + \\overrightarrow{-eur} = \\overrightarrow{chanteur} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0dac38",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* De nombreux travaux se penchent sur ou implémentent cet aspect (Lazaridou et al. 2013, Marelli et Baroni 2015, Boleda 2020)\n",
    "    * Une sous-branche particulière de la sémantique distributionnelle s'y consacre (*Compositional Distributional Semantics*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9adbf2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* On peut représenter le vecteur d'un élément complexe comme la moyenne des éléments qui le composent\n",
    "    * $\\overrightarrow{déjeuner} + \\overrightarrow{dîner} + \\overrightarrow{souper} + \\overrightarrow{goûter} + \\overrightarrow{bouffer} = \\overrightarrow{manger} $\n",
    "    * $\\overrightarrow{chanteur} + \\overrightarrow{danseur} + \\overrightarrow{chercheur} + \\overrightarrow{menteur} + \\overrightarrow{râleur} = \\overrightarrow{agent} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a949da",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Ces approches (inspirées de Kintsch 2001 notamment) ont permis de considérer, entre autres, les représentations prototypiques de :\n",
    "    * catégories sémantiques (Ho-Dac et al. 2020, Huyghe et Wauquier 2020)\n",
    "    * catégories morphologiques (Huyghe et Wauquier 2020, Wauquier 2020, Wauquier et Bonami (Under review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa90cbd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Elles permettent ainsi de traiter des sujets linguistiques aussi variées que :\n",
    "    * la délimitation d'une catégorie lexicale\n",
    "    * les propriétés distinctives d'affixes rivaux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f1ee9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Plus largement, les *word embeddings* peuvent s'utiliser dans des thématiques très variées\n",
    "    * Psycholinguistique\n",
    "    * Sociolinguistique (Miletic et al. 2021)\n",
    "    * Morphologie (Wauquier 2020)\n",
    "    * Analyse discursive (Ho-Dac et al. 2020)\n",
    "    * Sémantique (Huyghe et Wauquier 2020)\n",
    "    * Diachronie (Hamilton et al. 2016)\n",
    "    * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b45ec4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tâches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66a33a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Parce qu'ils correspondent à une information sémantique condensée, les *word embeddings* sont souvent intégrées dans des tâches variées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7f2ba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Parmi ses tâches, on trouve :\n",
    "    * le clustering\n",
    "    * la classification\n",
    "    * la prédiction\n",
    "    * la visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d1da7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Le clustering vise à regrouper ensemble des éléments de par leur proximité\n",
    "    * Huyghe et Wauquier (2020) regroupent des noms d'agent morphologiquement divers sur la base de leurs représentations distributionnelles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33aa84b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* La classification vise à l'attribution d'une étiquette (parmi un jeu d'étiquette) à un élément\n",
    "    * Wauquier et Bonami (under review) demandent à un système d'identifier s'il s'agit d'un nom en -*euse* ou -*rice* sur la base d'un vecteur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47fa2c7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* La prédiction consiste à prédire un vecteur sur la base de régularités observées\n",
    "    * Mickus et al. (2019) prédisent les vecteurs des formes féminines de noms et adjectifs masculins, et mesurent la distance entre le vecteur prédit et le vecteur réel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730043fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* La visualisation permet d'avoir une représentation de l'organisation spatiale des vecteurs\n",
    "\n",
    "<center>\n",
    " <img src=\"media/plot_100_voisins_barys_nhum.png\"  alt=\"visualisation vecteurs agent\" width=500/>\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10610bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Au-delà de Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a6238",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Word2Vec n'a été que le premier représentant de son genre\n",
    "    * De nombreuses variantes on depuis été proposées : doc2vec, word2vecf, dict2vec, nonce2vec..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a96398",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Une variante qui est relativement plebiscitée (bien que critiquée d'autre part) est fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fe28be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parenthèse sur fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1476ef9a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Comme son nom ne l'indique pas, fastText est supposé intégrer une composante 'morphologique'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c0407a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Ce système exploite en réalité le principe de compositionnalité \n",
    "    * Le mot est vu comme un ensemble de *n*-grammes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba580bc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Le vecteur du mot *chanteur* sera donc construit comme suit :\n",
    "    * $\\overrightarrow{chanteur} = \\overrightarrow{chanteur} + \\overrightarrow{cha} + \\overrightarrow{han} + \\overrightarrow{ant} + \\overrightarrow{nte} + \\overrightarrow{teu} + \\overrightarrow{eur} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d4b0c4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Cette approche est censée rendre cette représentation plus précise\n",
    "    * $\\overrightarrow{eur}$ est censé capter les propriétés distributionnelles moyennes des noms d'agent en -*eur*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e375792",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* En réalité, la composante 'morphologique' est davantage 'formelle'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3dc76d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* L'importance de la composante formelle nuit à la *relatedness* sémantique\n",
    "    * Cette approche permet cependant de produire des représentations pour des mots absents du corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d53ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Pour tester, vous pouvez charger le modèle fastText disponible via `gensim.downloader()` (SECTION 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71213cd4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fastText_model = gensim.downloader.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d44143",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Ce modèle s'utilise alors de la même façon que les modèles Word2Vec.\n",
    "    * Notons que l'entraînement d'un modèle fastText implique cependant des paramètres additionnels concernant la longueur des *n*-grammes par exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80dd278",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fastText_model.similarity('WORD1', 'WORD2')\n",
    "fastText_model.most_similar('WORD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a656b254",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parenthèse sur BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062e9198",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* L'utilisation des *word embeddings* a récemment connu une révolution avec l'arrivée d'*embeddings* dits contextuels\n",
    "    * Au lieu d'avoir un vecteur par forme, le système produit un vecteur par occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7c54f7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Cette approche permet de prendre en compte le contexte local\n",
    "    * Cela permet en théorie d'avoir des représentations différentes dans le cas de mots polysémiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa1d7d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Cette approche est cependant très lourde\n",
    "    * L'entraînement d'un modèle demande de très grosses ressources computationnelles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6445f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Références\n",
    "\n",
    "* Boleda, G. (2020). Distributional semantics and linguistic theory. *Annual Review of Linguistics*, 6, 213-234.\n",
    "* Kintsch W. (2001). Predication. *Cognitive science*, 25, 173–202.\n",
    "* Hamilton, W. L., Leskovec, J., & Jurafsky, D. (2016). Diachronic word embeddings reveal statistical laws of semantic change. *arXiv preprint arXiv:1605.09096*.\n",
    "* Ho-Dac, L. M., Miletic, A., Wauquier, M., & Fabre, C. (2020). Approches outillées pour l'étude des noms sous-spécifiés ou noms capsules. *Le Français Moderne*, (1).\n",
    "* Huyghe, R., & Wauquier, M. (2020). What’s in an agent?. *Morphology*, 30(3), 185-218."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f28c6f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Lazaridou, A., Marelli, M., Zamparelli, R., & Baroni, M. (2013, August). Compositional-ly derived representations of morphologically complex words in distributional semantics. In *Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, 1517-1526.\n",
    "* Lenci, A. (2018). Distributional models of word meaning. Annual review of Linguistics, 4, 151-171.\n",
    "* Marelli, M., & Baroni, M. (2015). Affixation in semantic space: Modeling morpheme meanings with compositional distributional semantics. *Psychological review*, 122(3), 485.\n",
    "* Mickus, T., Bonami, O., & Paperno, D. (2019). Distributional effects of gender contrasts across categories. In *Proceedings of the Society for Computation in Linguistics (SCiL) 2019*, 174-184."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33bb804",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Miletic, F., Przewozny-Desriaux, A., & Tanguy, L. (2021). Detecting Contact-Induced Semantic Shifts: What Can Embedding-Based Methods Do in Practice?. In *2021 Conference on Empirical Methods in Natural Language Processing*.\n",
    "* Wauquier, M. (2020). *Confrontation des procédés dérivationnels et des catégories sémantiques dans les modèles distributionnels*. Doctoral dissertation, Université Toulouse le Mirail-Toulouse II."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "livereveal": {
   "autolaunch": true,
   "scroll": true,
   "transition": "none"
  },
  "rise": {
   "overlay": "<div class='myheader'><h2>CUSO - Introduction à la sémantique distributionnelle</h2></div>",
   "theme": "moon",
   "width": "70%"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
